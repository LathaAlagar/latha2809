{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP40A/nQEEVInhOOnd6C1Eu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LathaAlagar/latha2809/blob/main/23BIT050_In_Memory_Data_Processing_Challenge_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhQa0tNeplQL",
        "outputId": "ade01f2d-1cd8-4397-b808-d4f47e557535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Spark session created\n",
            "✅ Synthetic dataset created with 1,000,000 rows\n",
            "✅ Spark DataFrame created\n",
            "+--------+------------------+-------------------+\n",
            "|category|        avg_amount|       total_amount|\n",
            "+--------+------------------+-------------------+\n",
            "|       B|252.36908405453337|6.295826303000039E7|\n",
            "|       D|252.79629827785737|6.326732956999937E7|\n",
            "|       C| 252.5064637945096|6.322862856000038E7|\n",
            "|       A|252.60327323228807| 6.31146960399998E7|\n",
            "+--------+------------------+-------------------+\n",
            "\n",
            "⏱ Execution time without cache: 13.52 seconds\n",
            "✅ DataFrame cached in memory\n",
            "+--------+------------------+-------------------+\n",
            "|category|        avg_amount|       total_amount|\n",
            "+--------+------------------+-------------------+\n",
            "|       B|252.36908405453337|6.295826303000039E7|\n",
            "|       D|252.79629827785737|6.326732956999937E7|\n",
            "|       C| 252.5064637945096|6.322862856000038E7|\n",
            "|       A|252.60327323228807| 6.31146960399998E7|\n",
            "+--------+------------------+-------------------+\n",
            "\n",
            "⏱ Execution time with cache: 2.35 seconds\n",
            "+-----------+-----------------+\n",
            "|customer_id|     total_amount|\n",
            "+-----------+-----------------+\n",
            "|      59778|7951.389999999999|\n",
            "|      19860|          7498.06|\n",
            "|      99106|7389.900000000001|\n",
            "|      23786|7125.830000000001|\n",
            "|      54313|7049.549999999999|\n",
            "+-----------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "⏱ Execution time for top customers query: 3.02 seconds\n",
            "✅ Spark session stopped\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, sum\n",
        "\n",
        "# ---------------------- Step 1: Initialize Spark ----------------------\n",
        "spark = SparkSession.builder.appName(\"InMemoryDataProcessing\").master(\"local[*]\").getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "print(\"✅ Spark session created\")\n",
        "\n",
        "# ---------------------- Step 2: Generate large dataset ----------------------\n",
        "N = 1000000  # 1 million rows\n",
        "data = pd.DataFrame({\n",
        "    \"customer_id\": [random.randint(1, 100000) for _ in range(N)],\n",
        "    \"transaction_amount\": [round(random.uniform(5, 500),2) for _ in range(N)],\n",
        "    \"category\": [random.choice([\"A\",\"B\",\"C\",\"D\"]) for _ in range(N)],\n",
        "    \"age\": [random.randint(18, 70) for _ in range(N)]\n",
        "})\n",
        "print(\"✅ Synthetic dataset created with 1,000,000 rows\")\n",
        "\n",
        "# ---------------------- Step 3: Create Spark DataFrame ----------------------\n",
        "df = spark.createDataFrame(data)\n",
        "print(\"✅ Spark DataFrame created\")\n",
        "\n",
        "# ---------------------- Step 4: Analytical query without caching ----------------------\n",
        "start = time.time()\n",
        "result1 = df.groupBy(\"category\").agg(avg(\"transaction_amount\").alias(\"avg_amount\"), sum(\"transaction_amount\").alias(\"total_amount\"))\n",
        "result1.show(5)\n",
        "end = time.time()\n",
        "print(f\"⏱ Execution time without cache: {end-start:.2f} seconds\")\n",
        "\n",
        "# ---------------------- Step 5: Cache DataFrame in memory ----------------------\n",
        "df.cache()\n",
        "df.count()  # Trigger caching\n",
        "print(\"✅ DataFrame cached in memory\")\n",
        "\n",
        "# ---------------------- Step 6: Analytical query with caching ----------------------\n",
        "start = time.time()\n",
        "result2 = df.groupBy(\"category\").agg(avg(\"transaction_amount\").alias(\"avg_amount\"), sum(\"transaction_amount\").alias(\"total_amount\"))\n",
        "result2.show(5)\n",
        "end = time.time()\n",
        "print(f\"⏱ Execution time with cache: {end-start:.2f} seconds\")\n",
        "\n",
        "# ---------------------- Step 7: Optional - Additional real-time analysis ----------------------\n",
        "# Example: top 5 customers by transaction amount\n",
        "start = time.time()\n",
        "top_customers = df.groupBy(\"customer_id\").agg(sum(\"transaction_amount\").alias(\"total_amount\")).orderBy(col(\"total_amount\").desc())\n",
        "top_customers.show(5)\n",
        "end = time.time()\n",
        "print(f\"⏱ Execution time for top customers query: {end-start:.2f} seconds\")\n",
        "\n",
        "# ---------------------- Step 8: Stop Spark ----------------------\n",
        "spark.stop()\n",
        "print(\"✅ Spark session stopped\")\n"
      ]
    }
  ]
}